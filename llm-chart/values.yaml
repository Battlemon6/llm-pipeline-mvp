backend:
  name: "vllm"

seedApp:
  replicaCount: 1
  image:
    repository: europe-west2-docker.pkg.dev/llm-pipeline-birkbeck/llm-app-images/seed-app
    tag: "c473aa8d7be5897fe4884bda3af6c1811da03242"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 8000
    metricsPort: 8000
  timeout: "120"
persistence:
  enabled: true
  size: 20Gi
  storageClassName: "premium-rwo"
vllm:
  replicaCount: 1
  image:
    repository: vllm/vllm-openai
    tag: "latest"
    pullPolicy: IfNotPresent
  model: "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
  quantization: "awq"
  gpuMemoryUtilization: 0.8
  service:
    name: vllm-server
    port: 8000
  nodeSelector:
    cloud.google.com/gke-accelerator: "nvidia-l4"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      nvidia.com/gpu: 1
    limits:
      nvidia.com/gpu: 1
tgi:
  replicaCount: 1
  image:
    repository: ghcr.io/huggingface/text-generation-inference
    tag: "1.4"
    pullPolicy: IfNotPresent
  model: "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
  service:
    name: tgi-server
    port: 80
    metricsPort: 8080
  nodeSelector:
    cloud.google.com/gke-accelerator: "nvidia-l4"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      nvidia.com/gpu: 1
      memory: "12Gi"
    limits:
      nvidia.com/gpu: 1
      memory: "16Gi"
