seedApp:
  replicaCount: 1
  image:
    repository: europe-west2-docker.pkg.dev/llm-pipeline-birkbeck/llm-app-images/seed-app
    tag: "ab432d730a9226bb18a7b049568424308428bc70"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 8000
    metricsPort: 8000
  env:
    VLLM_ENDPOINT: "http://vllm-server:8000"
    VLLM_MODEL: "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
    VLLM_TIMEOUT: "120"
vllm:
  replicaCount: 0
  image:
    repository: vllm/vllm-openai
    tag: "latest"
    pullPolicy: IfNotPresent
  model: "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
  quantization: "awq"
  gpuMemoryUtilization: 0.8
  persistence:
    enabled: true
    size: 20Gi
    storageClassName: "premium-rwo"
  service:
    port: 8000
    metricsPort: 8000
  nodeSelector:
    cloud.google.com/gke-accelerator: "nvidia-l4"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      nvidia.com/gpu: 1
    limits:
      nvidia.com/gpu: 1
