backend:
  name: "tgi"

seedApp:
  replicaCount: 1
  image:
    repository: europe-west2-docker.pkg.dev/llm-pipeline-birkbeck/llm-app-images/seed-app
    tag: "ab432d730a9226bb18a7b049568424308428bc70"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 8000
    metricsPort: 8000
  env:
    VLLM_ENDPOINT: "http://tgi-server:80"
    VLLM_MODEL: "mistralai/Mistral-7B-Instruct-v0.2"
    VLLM_TIMEOUT: "120"

persistence:
  enabled: true
  size: 20Gi
  storageClassName: "premium-rwo"

vllm:
  replicaCount: 0
  image:
    repository: vllm/vllm-openai
    tag: "latest"
    pullPolicy: IfNotPresent
  model: "mistralai/Mistral-7B-Instruct-v0.2"
  gpuMemoryUtilization: 0.8
  service:
    name: vllm-server
    port: 8000
    metricsPort: 8000
  nodeSelector:
    cloud.google.com/gke-accelerator: "nvidia-l4"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      nvidia.com/gpu: 1
    limits:
      nvidia.com/gpu: 1

tgi:
  replicaCount: 1
  image:
    repository: ghcr.io/huggingface/text-generation-inference
    tag: "1.4"
    pullPolicy: IfNotPresent
  model: "mistralai/Mistral-7B-Instruct-v0.2"
  service:
    name: tgi-server
    port: 80
    metricsPort: 8080
  nodeSelector:
    cloud.google.com/gke-accelerator: "nvidia-l4"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      nvidia.com/gpu: 1
    limits:
      nvidia.com/gpu: 1